{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "onGPU = True\n",
    "atCaltech = False\n",
    "onEOS = True\n",
    "gpuFraction = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "if onGPU:\n",
    "    import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# keras imports\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Conv2D, Dropout, Flatten, Concatenate, Reshape, BatchNormalization\n",
    "from keras.layers import MaxPooling2D, MaxPooling3D\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.regularizers import l1\n",
    "# hyperparameters\n",
    "import GPy, GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onGPU and gpuFraction<1.0:\n",
    "    # limit GPU usage\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpuFraction)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    import keras\n",
    "    from keras import backend as K\n",
    "    K.set_session(sess)\n",
    "    print('using gpu memory fraction: '+str(gpuFraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "# need afs password to read from eos\n",
    "import os\n",
    "import getpass\n",
    "if onEOS: os.system(\"echo %s| kinit\" %getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train and test samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/jetImage_30.h5\n",
      "(66000, 100, 100) (66000, 5)\n"
     ]
    }
   ],
   "source": [
    "target = np.array([])\n",
    "jetImage = np.array([])\n",
    "fileIN = \"../data/jetImage_30.h5\"\n",
    "print(fileIN)\n",
    "f = h5py.File(fileIN)\n",
    "X = np.array(f.get(\"jetImage\"))\n",
    "Y = np.array(f.get('jets')[0:,-6:-1])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44220, 100, 100) (21780, 100, 100) (44220, 5) (21780, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44220, 100, 100, 1) (21780, 100, 100, 1) (44220, 5) (21780, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "image_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import relu, selu, elu\n",
    "# myModel class\n",
    "class myModel():\n",
    "    def __init__(self, x_train, x_test, y_train, y_test, optmizer_index=0, CNN_filters=10, CNN_filter_size=5,\n",
    "                 CNN_layers=2, CNN_activation_index=0, DNN_neurons=40, DNN_layers=2, DNN_activation_index=0, \n",
    "                 dropout=0.2, batch_size=100, epochs=50):\n",
    "        self.activation = [relu, selu, elu]\n",
    "        self.optimizer = ['adam', 'nadam','adadelta']\n",
    "        self.optimizer_index = optmizer_index\n",
    "        self.CNN_filters = CNN_filters\n",
    "        self.CNN_filter_size = CNN_filter_size\n",
    "        self.CNN_layers = CNN_layers\n",
    "        self.CNN_activation_index = CNN_activation_index\n",
    "        self.DNN_neurons = DNN_neurons\n",
    "        self.DNN_layers = DNN_layers\n",
    "        self.DNN_activation_index = DNN_activation_index\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.__x_train, self.__x_test, self.__y_train, self.__y_test = x_train, x_test, y_train, y_test\n",
    "        self.__model = self.build()\n",
    "    \n",
    "    #  model\n",
    "    def build(self):\n",
    "        inputImage = Input(shape=(image_shape))\n",
    "        x = Conv2D(self.CNN_filters, kernel_size=(self.CNN_filter_size,self.CNN_filter_size), \n",
    "                   data_format=\"channels_last\", strides=(1, 1), padding=\"same\", \n",
    "                   activation= self.activation[self.CNN_activation_index], input_shape=image_shape,\n",
    "                    kernel_initializer='lecun_uniform', name='cnn2D_0_relu')(inputImage)\n",
    "        x = Dropout(self.dropout)(x)\n",
    "        for i in range(1,self.CNN_layers):\n",
    "            x = Conv2D(self.CNN_filters, kernel_size=(self.CNN_filter_size,self.CNN_filter_size), \n",
    "                   data_format=\"channels_last\", strides=(1, 1), padding=\"same\", \n",
    "                   activation= self.activation[self.CNN_activation_index], input_shape=image_shape,\n",
    "                    kernel_initializer='lecun_uniform', name='cnn2D_%i_relu' %i)(x)\n",
    "            x = Dropout(self.dropout)(x)\n",
    "        ####\n",
    "        x = Flatten()(x)\n",
    "        #\n",
    "        for i in range(self.DNN_layers):\n",
    "            x = Dense(self.DNN_neurons, activation=self.activation[self.DNN_activation_index], \n",
    "                      kernel_initializer='lecun_uniform', name='dense_%i_relu' %i)(x)\n",
    "            x = Dropout(self.dropout)(x)\n",
    "        #\n",
    "        output = Dense(5, activation='softmax', kernel_initializer='lecun_uniform', name = 'output_softmax')(x)\n",
    "        ####\n",
    "        model = Model(inputs=inputImage, outputs=output)\n",
    "        model.compile(optimizer=self.optimizer[self.optimizer_index], loss='categorical_crossentropy', metrics=['acc'])\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # fit model\n",
    "    def model_fit(self):\n",
    "        self.__model.fit(self.__x_train, self.__y_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=self.epochs,\n",
    "                        verbose=0,\n",
    "                        validation_data=[self.__x_test, self.__y_test],\n",
    "                        callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "                                    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0),\n",
    "                                    TerminateOnNaN()])       \n",
    "    \n",
    "    # evaluate mnist model\n",
    "    def model_evaluate(self):\n",
    "        self.model_fit()\n",
    "        evaluation = self.__model.evaluate(self.__x_test, self.__y_test, batch_size=self.batch_size, verbose=0)\n",
    "        return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner function for model\n",
    "# function to run mnist class\n",
    "\n",
    "def run_model(x_train, x_test, y_train, y_test, optmizer_index=0, CNN_filters=10, CNN_filter_size=5,\n",
    "                 CNN_layers=2, CNN_activation_index=0, DNN_neurons=40, DNN_layers=2, DNN_activation_index=0, \n",
    "                 dropout=0.2, batch_size=100, epochs=50):\n",
    "    \n",
    "    _model = myModel(x_train, x_test, y_train, y_test, optmizer_index, CNN_filters, CNN_filter_size,\n",
    "                 CNN_layers, CNN_activation_index, DNN_neurons, DNN_layers, DNN_activation_index, \n",
    "                 dropout, batch_size, epochs)\n",
    "    model_evaluation = _model.model_evaluate()\n",
    "    return model_evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bounds dict should be in order of continuous type and then discrete type\n",
    "bounds = [{'name': 'optmizer_index',        'type': 'discrete',  'domain': (0, 1, 2)},\n",
    "          {'name': 'CNN_filters',           'type': 'discrete',  'domain': (5, 10, 15, 20, 25, 30)},\n",
    "          {'name': 'CNN_filter_size',       'type': 'discrete',  'domain': (3, 5, 7, 9)},\n",
    "          {'name': 'CNN_layers',            'type': 'discrete',    'domain': (1, 2, 3, 4, 5)},\n",
    "          {'name': 'CNN_activation_index',  'type': 'discrete',    'domain': (0, 1, 2)},\n",
    "          {'name': 'DNN_neurons',           'type': 'discrete',    'domain': (10, 20, 30, 40, 50, 60, 70, 80, 90, 100)},\n",
    "          {'name': 'DNN_layers',            'type': 'discrete',    'domain': (1, 2, 3)},\n",
    "          {'name': 'DNN_activation_index',  'type': 'discrete',    'domain': (0, 1, 2)},\n",
    "          {'name': 'dropout',               'type': 'continuous',  'domain': (0.1, 0.4)},\n",
    "          {'name': 'batch_size',            'type': 'discrete',    'domain': (50, 100, 200, 500)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to optimize model\n",
    "def f(x):\n",
    "    print(x)\n",
    "    evaluation = run_model(X_train, X_test, Y_train, Y_test,\n",
    "        optmizer_index = int(x[:,0]), \n",
    "        CNN_filters = int(x[:,1]), \n",
    "        CNN_filter_size = int(x[:,2]),\n",
    "        CNN_layers = int(x[:,3]), \n",
    "        CNN_activation_index = int(x[:,4]), \n",
    "        DNN_neurons = int(x[:,5]), \n",
    "        DNN_layers = int(x[:,6]),\n",
    "        DNN_activation_index = int(x[:,7]),\n",
    "        dropout = float(x[:,8]),\n",
    "        batch_size = int(x[:,9]),\n",
    "        epochs = n_epochs)\n",
    "    print(\"LOSS:\\t{0} \\t ACCURACY:\\t{1}\".format(evaluation[0], evaluation[1]))\n",
    "    print(evaluation)\n",
    "    return evaluation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.50000000e+01 3.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 3.00000000e+01 2.00000000e+00 1.00000000e+00\n",
      "  1.00380733e-01 5.00000000e+02]]\n",
      "LOSS:\t0.7838268925335782 \t ACCURACY:\t0.73627180676596\n",
      "[0.7838268925335782, 0.73627180676596]\n",
      "[[2.0000000e+00 1.0000000e+01 7.0000000e+00 4.0000000e+00 1.0000000e+00\n",
      "  9.0000000e+01 1.0000000e+00 0.0000000e+00 3.9415882e-01 5.0000000e+02]]\n"
     ]
    }
   ],
   "source": [
    "# run optimization\n",
    "opt_model = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds)\n",
    "opt_model.run_optimization(max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optimized model\n",
    "print(\"\"\"\n",
    "Optimized Parameters:\n",
    "\\t{0}:\\t{1}\n",
    "\\t{2}:\\t{3}\n",
    "\\t{4}:\\t{5}\n",
    "\\t{6}:\\t{7}\n",
    "\\t{8}:\\t{9}\n",
    "\\t{10}:\\t{11}\n",
    "\\t{12}:\\t{13}\n",
    "\"\"\".format(bounds[0][\"name\"],opt_mnist.x_opt[0],\n",
    "           bounds[1][\"name\"],opt_mnist.x_opt[1],\n",
    "           bounds[2][\"name\"],opt_mnist.x_opt[2],\n",
    "           bounds[3][\"name\"],opt_mnist.x_opt[3],\n",
    "           bounds[4][\"name\"],opt_mnist.x_opt[4],\n",
    "           bounds[5][\"name\"],opt_mnist.x_opt[5],\n",
    "           bounds[6][\"name\"],opt_mnist.x_opt[6],\n",
    "           bounds[7][\"name\"],opt_mnist.x_opt[7],\n",
    "           bounds[8][\"name\"],opt_mnist.x_opt[8],\n",
    "           bounds[9][\"name\"],opt_mnist.x_opt[9]))\n",
    "print(\"optimized loss: {0}\".format(opt_mnist.fx_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_mnist.x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = open(\"OptCNN_30.log\", \"w\")\n",
    "# print optimized mnist model\n",
    "outFile.write(\"\"\"\n",
    "Optimized Parameters:\n",
    "\\t{0}:\\t{1}\n",
    "\\t{2}:\\t{3}\n",
    "\\t{4}:\\t{5}\n",
    "\\t{6}:\\t{7}\n",
    "\\t{8}:\\t{9}\n",
    "\\t{10}:\\t{11}\n",
    "\\t{12}:\\t{13}\n",
    "\"\"\".format(bounds[0][\"name\"],opt_mnist.x_opt[0],\n",
    "           bounds[1][\"name\"],opt_mnist.x_opt[1],\n",
    "           bounds[2][\"name\"],opt_mnist.x_opt[2],\n",
    "           bounds[3][\"name\"],opt_mnist.x_opt[3],\n",
    "           bounds[4][\"name\"],opt_mnist.x_opt[4],\n",
    "           bounds[5][\"name\"],opt_mnist.x_opt[5],\n",
    "           bounds[6][\"name\"],opt_mnist.x_opt[6],\n",
    "           bounds[7][\"name\"],opt_mnist.x_opt[7],\n",
    "           bounds[8][\"name\"],opt_mnist.x_opt[8],\n",
    "           bounds[9][\"name\"],opt_mnist.x_opt[9]))\n",
    "outFile.write(\"\\n\")\n",
    "outFile.write(\"optimized loss: {0}\".format(opt_mnist.fx_opt))\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
